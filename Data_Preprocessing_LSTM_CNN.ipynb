{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b53bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793a8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/preprocessed_general.pkl', 'rb') as f:\n",
    "    chosen_stocks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9264f32",
   "metadata": {},
   "source": [
    "We now have our stock data with features for each stock. The next step for us is to split the data into train and test/validation to fit a scaler only on the training set.\n",
    "We do that so that the scaler does not get any information about the range the values may lie in in the test or validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d525ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-13 00:00:00\n",
      "2024-11-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "end_train_set = chosen_stocks[0][0].index[int((chosen_stocks[0][0].shape[0] - 1) * 0.8)]\n",
    "end_validation_set = chosen_stocks[0][0].index[int((chosen_stocks[0][0].shape[0] - 1) * 0.9)]\n",
    "print(end_train_set)\n",
    "print(end_validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd9d77",
   "metadata": {},
   "source": [
    "*** WARNING *** This Pipeline somehow confuses the column symbol with usd_jpy. Still to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to hold the data all bins\n",
    "preprocessed_NN_data = []\n",
    "for i in range(len(chosen_stocks)):\n",
    "    preprocessed_NN_bin = []\n",
    "    for j in range(len(chosen_stocks[i])):\n",
    "        #Split into train, validation and test sets (data is already sorted by date)\n",
    "        X_train = chosen_stocks[i][j].loc[:end_train_set, chosen_stocks[i][j].columns != 'target']\n",
    "        y_train = chosen_stocks[i][j].loc[:end_train_set, 'target']\n",
    "        X_validation = chosen_stocks[i][j].loc[end_train_set:end_validation_set, chosen_stocks[i][j].columns != 'target']\n",
    "        y_validation = chosen_stocks[i][j].loc[end_train_set:end_validation_set, 'target']\n",
    "        X_test = chosen_stocks[i][j].loc[end_validation_set:, chosen_stocks[i][j].columns != 'target']\n",
    "        y_test = chosen_stocks[i][j].loc[end_validation_set:, 'target']\n",
    "\n",
    "        # Scale Features based on train set\n",
    "        # We drop the 'symbol' column before scaling and store it seperately\n",
    "        symbol = X_train.iloc[0]['symbol']\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train.drop(columns=['symbol'], inplace=False))\n",
    "        X_validation_scaled = scaler.transform(X_validation.drop(columns=['symbol'], inplace=False))\n",
    "        X_test_scaled = scaler.transform(X_test.drop(columns=['symbol'], inplace=False))\n",
    "\n",
    "        # Put back into dfs so we have the original indices and columns for windowing\n",
    "        columns = X_train.drop(columns=['symbol']).columns\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=columns)\n",
    "        X_validation_scaled = pd.DataFrame(X_validation_scaled, index=X_validation.index, columns=columns) \n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=columns)\n",
    "\n",
    "        # We concatenate the dataframes to create a single dataset for windowing so that we dont lose window.size() windows at the start of each set but only once.\n",
    "        X = pd.concat([X_train_scaled, X_validation_scaled, X_test_scaled])\n",
    "        y = pd.concat([y_train, y_validation, y_test])\n",
    "\n",
    "        entry = (X, y, symbol)\n",
    "        preprocessed_NN_bin.append(entry)\n",
    "    preprocessed_NN_data.append(preprocessed_NN_bin)\n",
    "\n",
    "# Save the data\n",
    "with open(f'preprocessed_data/preprocessed_LSTM_CNN.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_NN_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0e8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/preprocessed_LSTM_CNN.pkl', 'rb') as f:\n",
    "    LSTM_CNN_preprocessed_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f3a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981, 61) (123, 61) (124, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_validation.shape, X_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
