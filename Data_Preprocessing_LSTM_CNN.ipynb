{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load generally preprocessed data\n",
    "with open('preprocessed_data/preprocessed_general.pkl', 'rb') as f:\n",
    "    chosen_stocks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9264f32",
   "metadata": {},
   "source": [
    "We now have our stock data with features for each stock. The next step for us is to split the data into train and test/validation to fit a scaler only on the training set.\n",
    "We do that so that the scaler does not get any information about the range the values may lie in in the test or validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d525ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-13 00:00:00\n",
      "2024-11-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# we split after dates\n",
    "end_train_set = chosen_stocks[0][0].index[int((chosen_stocks[0][0].shape[0] - 1) * 0.8)]\n",
    "end_validation_set = chosen_stocks[0][0].index[int((chosen_stocks[0][0].shape[0] - 1) * 0.9)]\n",
    "print(end_train_set)\n",
    "print(end_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outer list to store preprocessed data per bin\n",
    "preprocessed_NN_data = []\n",
    "for i in range(len(chosen_stocks)):\n",
    "    # inner lest to store preprocessed data per asset\n",
    "    preprocessed_NN_bin = []\n",
    "    for j in range(len(chosen_stocks[i])):\n",
    "        #Split into train 0.9, validation 0.1 and test 0.1 sets\n",
    "        X_train = chosen_stocks[i][j].loc[:end_train_set].drop(columns=['target', 'target_ret', 'target_ret_log'])\n",
    "        y_train = chosen_stocks[i][j].loc[:end_train_set, 'target_ret_log']\n",
    "        X_validation = chosen_stocks[i][j].loc[end_train_set:end_validation_set].drop(columns=['target', 'target_ret', 'target_ret_log'])\n",
    "        y_validation = chosen_stocks[i][j].loc[end_train_set:end_validation_set, 'target_ret_log']\n",
    "        X_test = chosen_stocks[i][j].loc[end_validation_set:].drop(columns=['target', 'target_ret', 'target_ret_log'])\n",
    "        y_test = chosen_stocks[i][j].loc[end_validation_set:, 'target_ret_log']\n",
    "\n",
    "\n",
    "        # Scale Features based on train set\n",
    "        # We drop the 'symbol' column before scaling and store it seperately\n",
    "        symbol = X_train.iloc[0]['symbol']\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train.drop(columns=['symbol'], inplace=False))\n",
    "        X_validation_scaled = scaler.transform(X_validation.drop(columns=['symbol'], inplace=False))\n",
    "        X_test_scaled = scaler.transform(X_test.drop(columns=['symbol'], inplace=False))\n",
    "\n",
    "        # Put back into dfs so we have the original indices and columns for windowing\n",
    "        columns = X_train.drop(columns=['symbol']).columns\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=columns)\n",
    "        X_validation_scaled = pd.DataFrame(X_validation_scaled, index=X_validation.index, columns=columns) \n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=columns)\n",
    "\n",
    "        # We concatenate the dataframes to create a single dataset for windowing so that we dont lose window.size() windows at the start of each set but only once.\n",
    "        X = pd.concat([X_train_scaled, X_validation_scaled, X_test_scaled])\n",
    "        y = pd.concat([y_train, y_validation, y_test])\n",
    "\n",
    "        # store the preprocessed data along with the symbol\n",
    "        entry = (X, y, symbol)\n",
    "        preprocessed_NN_bin.append(entry)\n",
    "    preprocessed_NN_data.append(preprocessed_NN_bin)\n",
    "\n",
    "# write out the preprocessed data\n",
    "with open(f'preprocessed_data/preprocessed_LSTM_CNN.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_NN_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f3a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981, 61) (123, 61) (124, 61)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_validation.shape, X_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
